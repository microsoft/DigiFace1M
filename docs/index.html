<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DigiFace-1M</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mixed Reality & AI Lab – Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mixed Reality & AI
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://wacv2023.thecvf.com/">
                        <img class="is-hidden-touch" src="img/wacv-logo-light.png" style="height: 100%;" alt="WACV 2023">
                        <img class="is-hidden-desktop" src="img/wacv-logo-dark.png" style="height: 100%;" alt="WACV 2023">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 is-size-3-mobile is-spaced has-text-centered">
                DigiFace-1M: 1 Million Digital Face Images for Face Recognition
            </h1>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                Winter Conference on<br class="is-hidden-tablet"> Applications of Computer Vision 2023
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">

                <span>
                    <a href="https://www.baegwangbin.com">Gwangbin&nbsp;Bae</a>
                </span>
                <span>
                    <a href="mailto:martin.delagorce@microsoft.com">Martin&nbsp;de&nbsp;La&nbsp;Gorce</a>
                </span>
                <span>
                    <a href="mailto:tabaltru@microsoft.com">Tadas&nbsp;Baltrušaitis</a>
                </span>
                <span>
                    <a href="https://chewitt.me/">Charlie&nbsp;Hewitt</a>
                </span>
                <span>
                    <a href="mailto:doch@microsoft.com">Dong&nbsp;Chen</a>
                </span>
                <span>
                    <a href="mailto:juvalen@microsoft.com">Julien&nbsp;Valentin</a>
                </span>
                <span>
                    <a href="mailto:rc10001@cam.ac.uk">Roberto&nbsp;Cipolla</a>
                </span>
                <span>
                    <a href="mailto:jinshen@microsoft.com">Jingjing&nbsp;Shen</a>
                </span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a href="https://openaccess.thecvf.com/content/WACV2023/html/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.html" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2210.02579" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="https://github.com/microsoft/DigiFace1M" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image">
                <img src="img/sx-data.jpg" class="is-hidden-mobile"/>
                <img src="img/sx-data-narrow.jpg" class="is-hidden-tablet" />
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset.
                However, these models are trained on large-scale datasets that contain millions of real human face images collected from the internet.
                Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain labeling noise.
                Most importantly, these face images are collected without explicit consent, raising more pressing privacy and ethical concerns.
                To avoid the problems associated with real face datasets, we introduce a large-scale synthetic dataset for face recognition, obtained by photo-realistic rendering of diverse and high-quality digital faces using a computer graphics pipeline.
                We compare our method to SynFace, a recent method trained on GAN-generated synthetic faces, and reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%).
                We first demonstrate that aggressive data augmentation can significantly help reduce the domain-gap between our synthetic faces and real face images.
                Taking advantage of having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories, and textures) affects the accuracy.
                Finally, by fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images, while alleviating the problems associated with large datasets.
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Motivation
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    State-of-the-art face recognition models are trained on millions of real human face images collected from the internet.
                    <b>DigiFace-1M</b> aims to tackle three major problems associated with such large-scale face recognition datasets.
                </p>
                <ul>
                    <li>
                        <b>Ethical issues</b> -
                        Many existing datasets are obtained by collecting web images without explicit consent.
                        Our digital faces are created using a generative model built from high quality head scans of a small number of individuals <i>obtained with consent</i>.
                    </li>
                    <li>
                        <b>Labeling noise</b> -
                        Web images collected by searching the names of celebrities often contain errors.
                        Our synthetic data has <i>guaranteed correctness of labels</i>.
                    </li>
                    <li>
                        <b>Data bias</b> -
                        Face recognition models are generally trained and tested on celebrity faces, many of which are taken with strong lighting and make-up.
                        They also have imbalanced racial distribution.
                        Our synthetic data generation pipeline allows us to control the distribution of the data and ensure a <i>fair</i> dataset.
                    </li>
                </ul>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                About the Dataset
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    We build on the synthetic face generation framework of <a href="https://microsoft.github.io/FaceSynthetics/">Wood et al.</a> to create a dataset of <b>over one million</b> synthetic face images.
                    This synthetics dataset helps us to overcome three primary shortcomings of existing large-scale face recognition datasets:
                </p>
                <p>
                    We define identity as a unique combination of facial geometry, texture, eye color and hair style.
                    For each identity, we sample a set of accessories including clothing, make-up, glasses, face-wear and head-wear.
                </p>
            </div>
            <div class="columns">
                <div class="column">
                    <img src="img/accessories_id1.jpg" />
                </div>
                <div class="column is-hidden-mobile">
                    <img src="img/accessories_id2.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    While hair style can change for an individual, most people maintain similar hair style (for both facial and head hair) which makes hair style an important cue for the person's identity.
                    Therefore, for the same identity, we randomize only the color, density and thickness of the hair (top row) and avoiding the impression of changing identity (bottom row).
                    This simulates aging to some extent as hair typically becomes grayer, sparser and thinner during aging.
                    The hair style is only changed when the added head-wear is not compatible with the original hair style.
                </p>
            </div>
            <div class="columns">
                <div class="column" style="padding-right: 0">
                    <img src="img/hair1.jpg" />
                </div>
                <div class="column is-hidden-mobile" style="padding-left: 0">
                    <img src="img/hair2.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    After sampling the identity and the accessories, we can render multiple images by varying the pose, expression, environment (lighting and background) and camera.
                </p>
            </div>
            <div class="columns">
                <div class="column">
                    <img src="img/variety_id1.jpg" />
                </div>
                <div class="column is-hidden-mobile">
                    <img src="img/variety_id2.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    <b>DigiFace-1M</b> is split into two parts:
                    <ul>
                        <li>720K images with 10K identities (72 images per identity). For each identity, 4 different sets of accessories are sampled and 18 images are rendered for each set.</li>
                        <li>500K images with 100K identities (5 images per identity). For each identity, only one set of accessories is sampled.</li>
                    </ul>
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Results
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Qiu_SynFace_Face_Recognition_With_Synthetic_Data_ICCV_2021_paper.html">SynFace</a> is the current state-of-the-art for face recognition model trained on synthetic faces.
                    They used <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.pdf">DiscoFaceGAN</a> to generate 500K synthetic faces of 10K unique identities.
                    We significantly outperform SynFace across all datasets, suggesting that our rendered synthetic faces are better than GAN-generated faces for learning face recognition.
                    This is likely because GAN-generated images do not enforce identity or geometric consistency, and are not effective at changing accessories.
                    The GAN models also have unresolved ethical and bias concerns as they are typically trained on large-scale real face datasets.
                </p>
                <table class="table">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>#images</th>
                            <th class="is-hidden-mobile">LFW</th>
                            <th class="is-hidden-mobile">CFP-FP</th>
                            <th class="is-hidden-mobile">CPLFW</th>
                            <th class="is-hidden-mobile">AgeDB</th>
                            <th class="is-hidden-mobile">CALFW</th>
                            <th>Avg</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Qiu_SynFace_Face_Recognition_With_Synthetic_Data_ICCV_2021_paper.html">SynFace</a></td>
                            <td>500K (10K&#215;50)</td>
                            <td class="is-hidden-mobile">91.93</td>
                            <td class="is-hidden-mobile">75.03</td>
                            <td class="is-hidden-mobile">70.43</td>
                            <td class="is-hidden-mobile">61.63</td>
                            <td class="is-hidden-mobile">74.73</td>
                            <td>74.75</td>
                        </tr>
                        <tr>
                            <td>Ours</td>
                            <td>500K (10K&#215;50)</td>
                            <td class="is-hidden-mobile">95.40</td>
                            <td class="is-hidden-mobile">87.40</td>
                            <td class="is-hidden-mobile">78.87</td>
                            <td class="is-hidden-mobile">76.97</td>
                            <td class="is-hidden-mobile">78.62</td>
                            <td>83.45</td>
                        </tr>
                        <tr>
                            <td>Ours</td>
                            <td>1.22M (10K&#215;72&#43;100K&#215;5)</td>
                            <td class="is-hidden-mobile"><b>95.82</b></td>
                            <td class="is-hidden-mobile"><b>88.77</b></td>
                            <td class="is-hidden-mobile"><b>81.62</b></td>
                            <td class="is-hidden-mobile"><b>79.72</b></td>
                            <td class="is-hidden-mobile"><b>80.70</b></td>
                            <td><b>85.32</b></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    When a small number of real face images are available, we can use them to fine-tune the network that is pre-trained on our synthetic data.
                    Such fine-tuning significantly improves the accuracy across all datasets.
                </p>
            </div>
            <div class="columns">
                <div class="column has-text-centered">
                    <img src="img/mix_real_lfw.jpg" />
                </div>
                <div class="column is-hidden-mobile has-text-centered">
                    <img src="img/mix_real_cdp-fp.jpg" />
                </div>
                <div class="column is-hidden-mobile has-text-centered">
                    <img src="img/mix_real_cplfw.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    However, there remains a substantial accuracy gap from the state-of-the-art methods that are trained on large-scale real face datasets.
                    This gap can be reduced by adopting better data augmentation or by improving the realism of the face generation pipeline.
                    We leave this as future work.
                </p>
                <table class="table">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>#synth images</th>
                            <th>#real images</th>
                            <th class="is-hidden-mobile">LFW</th>
                            <th class="is-hidden-mobile">CFP-FP</th>
                            <th class="is-hidden-mobile">CPLFW</th>
                            <th class="is-hidden-mobile">AgeDB</th>
                            <th class="is-hidden-mobile">CALFW</th>
                            <th>Avg</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Ours</td>
                            <td>1.22M</td>
                            <td>0</td>
                            <td class="is-hidden-mobile">96.17</td>
                            <td class="is-hidden-mobile">89.81</td>
                            <td class="is-hidden-mobile">82.23</td>
                            <td class="is-hidden-mobile">81.10</td>
                            <td class="is-hidden-mobile">82.55</td>
                            <td>86.37</td>
                        </tr>
                        <tr>
                            <td>Ours + Real</td>
                            <td>1.22M</td>
                            <td>120K</td>
                            <td class="is-hidden-mobile">99.33</td>
                            <td class="is-hidden-mobile">95.93</td>
                            <td class="is-hidden-mobile">89.47</td>
                            <td class="is-hidden-mobile">91.55</td>
                            <td class="is-hidden-mobile">91.78</td>
                            <td>93.61</td>
                        </tr>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html">CosFace</a></td>
                            <td>0</td>
                            <td>5.8M</td>
                            <td class="is-hidden-mobile">99.78</td>
                            <td class="is-hidden-mobile">98.26</td>
                            <td class="is-hidden-mobile">92.18</td>
                            <td class="is-hidden-mobile"><b>98.17</b></td>
                            <td class="is-hidden-mobile"><b>96.18</b></td>
                            <td>96.91</td>
                        </tr>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Meng_MagFace_A_Universal_Representation_for_Face_Recognition_and_Quality_Assessment_CVPR_2021_paper.html">MagFace</a></td>
                            <td>0</td>
                            <td>5.8M</td>
                            <td class="is-hidden-mobile"><b>99.83</b></td>
                            <td class="is-hidden-mobile">98.46</td>
                            <td class="is-hidden-mobile">92.87</td>
                            <td class="is-hidden-mobile"><b>98.17</b></td>
                            <td class="is-hidden-mobile">96.15</td>
                            <td>97.10</td>
                        </tr>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_AdaFace_Quality_Adaptive_Margin_for_Face_Recognition_CVPR_2022_paper.html">AdaFace</a></td>
                            <td>0</td>
                            <td>5.8M</td>
                            <td class="is-hidden-mobile">99.82</td>
                            <td class="is-hidden-mobile"><b>98.49</b></td>
                            <td class="is-hidden-mobile"><b>93.53</b></td>
                            <td class="is-hidden-mobile">98.05</td>
                            <td class="is-hidden-mobile">96.08</td>
                            <td><b>97.19</b></td>
                        </tr>
                    </tbody>
                </table>
            </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@inproceedings{bae2023digiface1m,
    title={DigiFace-1M: 1 Million Digital Face Images for Face Recognition},
    author={Bae, Gwangbin and de La Gorce, Martin and Baltru{\v{s}}aitis, Tadas and Hewitt, Charlie and Chen, Dong and Valentin, Julien and Cipolla, Roberto and Shen, Jingjing},
    booktitle={2023 IEEE Winter Conference on Applications of Computer Vision (WACV)},
    year={2023},
    organization={IEEE}
}
</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at <a
                    href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mesh Labs &ndash;
                    Cambridge</a>.<br />
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2024</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
